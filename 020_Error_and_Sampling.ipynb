{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinkplot\n",
    "import thinkstats2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "import random\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE from residuals\n",
    "def rmseFromResiduals(residuals):\n",
    "    SSE = 0\n",
    "    for i in range(len(residuals)):\n",
    "        SSE += (residuals[i]**2)\n",
    "    MSE = SSE/len(residuals)\n",
    "    return math.sqrt(MSE)\n",
    "def rSquared(yvals, residuals):\n",
    "    return (1-(thinkstats2.Var(residuals)/thinkstats2.Var(yvals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RMSE and R2 - A Second Look</h1>\n",
    "\n",
    "Previously we calculated the RMSE, our measure of how much we can expect each prediction to be off by. However, the purpose of doing these predictions isn't to analyze the data we already have, it is to make predictions for new data. So when we are faced with brand new, never seen before data, how confident should we be in the accuracy of our models?\n",
    "\n",
    "When we just calculated the RMSE, we calculated the residuals for the data we used to create the model. We should kind of expect that the model we end up with would be pretty good at making predictions of the data that was used to create it!\n",
    "\n",
    "In order to get a more durable metric of error, we really need to test our model with some new data that it hasn't seen before. This will give us a better estimate of what type of accuracy we should expect when we put our model to use making real predictions.\n",
    "\n",
    "How? We will hold out some data when we make the model, then use that saved data as though it were new. Since we know the real answers, we can calculate our error metrics and compare them to the true answers. \n",
    "\n",
    "So, new process:\n",
    "<ul>\n",
    "<li>Split our data into two parts. \n",
    "<li>Use one part to calculate a linear regression. (Train the model)\n",
    "<li>Use the other part as a test - make predictions using the X values.\n",
    "<li>Compare those predictions to the true Y values from the held-out data, use these residuals for accuracy. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"brain.csv\")\n",
    "df = df.rename(columns={\"Head Size(cm^3)\":\"X\", \"Brain Weight(grams)\":\"Y\"})\n",
    "x = \"X\"\n",
    "y = \"Y\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot to take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNS Regression Plot\n",
    "sns.regplot(data=df, x=x, y=y, ci=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the regression and calculate the original RMSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "inter, slope = thinkstats2.LeastSquares(df[x],df[y])\n",
    "res = thinkstats2.Residuals(df[x], df[y], inter, slope)\n",
    "regLine = thinkstats2.FitLine(df[x], inter, slope)\n",
    "print(\"Y intercept:\", inter)\n",
    "print(\"Slope:\", slope)\n",
    "#RMSE/R2\n",
    "print(\"RMSE\", rmseFromResiduals(res))\n",
    "print(\"R2\", rSquared(df[y], res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>We have results, but are they good?</h3>\n",
    "\n",
    "These are our measures of error. However, we're caclulating our model's error based on the data that it 'learned' from. It is kind of like predicting test performance by just giving someone a test to study, then giving it to them again to write. Not really all that portable. \n",
    "\n",
    "A better aproach is to test the predictions from our regression on brand new data, that we didn't use to train it. In other words, we can take one sample of our data to train our model, and another to test it. \n",
    "\n",
    "We will use 70% of the data to train the model, and the remaining 30% to test it. This is a pretty common split. In general, the more data you have, the smaller the percentage you need to test with. Somewhere around two-thirds training and one-third testing is a good starting point in most situations. \n",
    "\n",
    "We can try...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into two random segments. \n",
    "\n",
    "#Calculate the number of rows for the split\n",
    "\n",
    "#Create split using sample(), which is random\n",
    "\n",
    "\n",
    "#Results\n",
    "print(\"Original Length:\", len(df))\n",
    "print(\"Train Length:\", len(train))\n",
    "print(\"Test Length:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two separate datasets - we can make a regression with one set, then test it on the other set!\n",
    "\n",
    "The model creation part is the same, except we are using the training subset, not the entirety of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "interTr, slopeTr = thinkstats2.LeastSquares(train[x],train[y])\n",
    "print(\"Y intercept:\", interTr)\n",
    "print(\"Slope:\", slopeTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the results are somewhat different than the original - that is because we are only using some of the data to generate the regression model. If we rerun that splitting step before, each one will be a little different because the data that we use changes. How much this variation is depends on the dispersion of the data and the total amount of data - more data, more stable values. \n",
    "\n",
    "The testing/error calculation step is different. Now instead of evaluating the accuracy against the original data we used, we want to take our new leftover X values in the testing dataset, generate predictions for their Y values, and compare those predictions to the real Y values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how far the held our data points are from the predictions to generate residuals\n",
    "\n",
    "#Get the residuals into a var named resTest. \n",
    "#RMSE/R2 calculations\n",
    "print(\"RMSE\", rmseFromResiduals(resTest))\n",
    "print(\"R2\", rSquared(test[y], resTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Updated Error Metrics</h3>\n",
    "\n",
    "These results for RMSE and R2 are generated by actually testing the model, with data that is new to it. This is much more realistic to actual, real life use. If we are actually predicting brain size, we are going to be measuring a stream of new skulls, taking that value, and plugging it in to our regression to get a predicted brain size. That's exactly what we did here, but we have those real brain sizes that we can pull out after to calculate the accuracy. \n",
    "\n",
    "<br><br><br>\n",
    "<h2>Repeated Trials</h2>\n",
    "\n",
    "What if we were to repeat this a entire process few times? And tally up the results? \n",
    "\n",
    "We'll also add in proper documentation to this function, since we're fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple samples and regressions.\n",
    "\n",
    "def repeatedRegression(data, trials=1000, trainSplit=.7, xColName=\"X\", yColName=\"Y\"):\n",
    "    \"\"\"Perform multiple linear regressions and accumulate the results in lists.\n",
    "\n",
    "    data: DataFrame of data\n",
    "    trials: number of test runs to perform\n",
    "    trainSplit: the percentage of data to use for model creation, the rest is held for testing. \n",
    "    xColName: the column name of the X data in the dataframe \"data\". \n",
    "    yColName: the column name of the X data in the dataframe \"data\".\n",
    "\n",
    "    returns: intercepts, slopes, rmses, r2s\n",
    "    \"\"\"\n",
    "    #Store each result in these lists. \n",
    "    intercepts = []\n",
    "    slopes = []\n",
    "    rmses = []\n",
    "    r2s = []\n",
    "\n",
    "    #Calculate the split size metrics\n",
    "    totalRows = len(data)\n",
    "    trainRows = round(totalRows*trainSplit)\n",
    "    testRows = totalRows-trainRows\n",
    "\n",
    "    for i in range(trials):\n",
    "        #Randomly split the data into training and testing\n",
    "\n",
    "        #Do a Regression\n",
    "\n",
    "        #print(\"Y intercept:\", inter, \"Slope:\", slope)\n",
    "        #Generate Residuals\n",
    "\n",
    "        #RMSE/R2\n",
    "\n",
    "        #print(\"RMSE\", rmse, \"R2\", r2)\n",
    "        #Add all the results to the lists \n",
    "\n",
    "        \n",
    "    return intercepts, slopes, rmses, r2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run it for our data. \n",
    "intercepts, slopes, rmses, r2s = repeatedRegression(df,10000)\n",
    "np.mean(intercepts), np.mean(slopes), np.mean(rmses), np.mean(r2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to any of the estimation trials that we previously looked at, we just do the predicting over and over, and count up the results. \n",
    "\n",
    "We can plot all of these results to take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot each set of results\n",
    "thinkplot.PrePlot(2,2,2)\n",
    "sns.histplot(intercepts)\n",
    "thinkplot.SubPlot(2)\n",
    "sns.histplot(slopes)\n",
    "thinkplot.SubPlot(3)\n",
    "sns.histplot(rmses)\n",
    "thinkplot.SubPlot(4)\n",
    "sns.histplot(r2s)\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Repetitive Results</h3>\n",
    "\n",
    "We end up with a distribution of results for each of our metrics. Like before, these distributions are descriptions of how likely the statistic is to fall at certain values. So we can do things like plot them in a CDF to look for confidence intervals. We can also analyze them to make sense of the regression - the less dispersed these values are, the more reliable our regression is likely to be - we get fewer changes depending on what subset of data we select. \n",
    "\n",
    "This particular dataset is pretty small, so we see some pretty large variation between trials. If we were to only do one split, we might randomly end up with one of the ones that had an RMSE or R2 way on the fringes of what is possible, and we'd not get an accurate assessment of our model's predictive quality. As the amount of data increases, this variation lessens. \n",
    "\n",
    "We can use the function from chapter 10 of the book to plot confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Book stuff to plot. \n",
    "def FitLine(xs, inter, slope):\n",
    "    fit_xs = np.sort(xs)\n",
    "    fit_ys = inter + slope * fit_xs\n",
    "    return fit_xs, fit_ys\n",
    "    \n",
    "def PlotConfidenceIntervals(xs, inters, slopes, percent=90, **options):\n",
    "    fys_seq = []\n",
    "    for inter, slope in zip(inters, slopes):\n",
    "        fxs, fys = FitLine(xs, inter, slope)\n",
    "        fys_seq.append(fys)\n",
    "\n",
    "    p = (100 - percent) / 2\n",
    "    percents = p, 100 - p\n",
    "    low, high = thinkstats2.PercentileRows(fys_seq, percents)\n",
    "    thinkplot.FillBetween(fxs, low, high, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call PCI with our data. Graph alongside SNS\n",
    "#These should be close, but they may differ slightly. The regplot does its own regression, so it may randomly vary. \n",
    "thinkplot.PrePlot(2,1,2)\n",
    "PlotConfidenceIntervals(df[x], intercepts, slopes)\n",
    "thinkplot.SubPlot(2)\n",
    "sns.regplot(x=df[x], y=df[y], ci=90, scatter=False)\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As an exercise, do it the other way around. \n",
    "#X = brain weight.\n",
    "#Y = head size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Weighted Resampling</h1>\n",
    "\n",
    "Up until now, we've treated every row of data equally, however, in certain situations, that may not give us what we need. As an example, think about detecting fraud in a credit card transaction. A credit card company has a massive amount of data that they can (and do) use to train machine learning algorithms to look for fraudulent transactions. \n",
    "\n",
    "However, almost all of that data will NOT be fraudulent. There are millions and millions of legitimate transactions and while fraud isn't really rare, it does only happen in a small minority of cases. The problem this can cause when building models is that we might just not have enough examples of the rare thing (fraud) for our algorithm to accurately learn from. A similar thing can happen with medical data - if there are conditions that impact specific subsets of society differently (e.g. sickle cell is more common in black people, people from Northern Europe are more likely to tolerate lactose) then we might not have enough of those examples to generate reliable predictions, especially when sample sizes are limited by cost and when we are slicing our data into segments to build the algorithms. \n",
    "\n",
    "There are a few ways that we can attempt to deal with this, the simplest being to weight the samples - or pull more results from some subsets of data than others. For example, we can purposefully take a greater share of the fraudulent transactions to ensure that we have enough. In a medical example you'd make sure that there was a significant number of each subgroup in the data, to ensure that they aren't 'missed'.\n",
    "\n",
    "<b>But wait, doesn't this mess up everything?</b>\n",
    "\n",
    "In some ways yes, in others no. We aren't using the data here to look at the distribution or similar - the number of frauds/not frauds in the data isn't really something we care about. We are attempting to learn how to predict the outcome (fraud/not) given the input(s) - so we need to ensure that there's enough examples so that we can do so. If fraud makes up .1% of the data, or 50% of the data, that doesn't really impact what we're doing negatively. \n",
    "\n",
    "The only real metric for success is our accuracy in making those predictions, so if we need to distort data to get better predictions, that's fine. \n",
    "\n",
    "<b>This makes more sense in multiple regression, so we'll save it for then</b>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
