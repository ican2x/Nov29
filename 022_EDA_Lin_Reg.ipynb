{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# Importing Libraries\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n"]},{"cell_type":"markdown","metadata":{},"source":["<H1>EDA and Regression</h1>\n","\n","Here we will walk through doing a regression for a common example dataset, complete with walking through some common exploratory steps. This is representative of what we'd do in a real situation, just a simple version. As we go through this we want to:\n","\n","<ul>\n","<li>Load and clean the data - make sure we've gotten rid of junk, fixed errors and blanks, corrected data type issues, etc...\n","<li>Explore the data - what does our data look like? Do we have consistent data? What are the distributions and correlations? Is there anything that may make us adjust or correct before proceeding?\n","<li>Shape the data for prediction - prep whatever data we are going to use in a regression ready format.\n","<li>Perform regression.\n","<li>Examine results\n","</ul>\n","\n","For the cleaning and exploring steps especially, we aren't really following a specific set of actions. We want to look at the data, and see if there's anything to change - some data may need cleaning, some won't; some data may have features we want to remove or change, some won't. We basically just want to look for anything that might lead us to adjust our approach away from just using all the data unchanged - this example isn't super dirty/complex, so we won't be doing an overwhelming amount of action here. This process is something we get better at with time, practice, and more ML tools. "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["# Importing the Dataset\n","df = pd.read_csv(\"auto-mpg.data\",delim_whitespace=True, names=[\"MPG\",\"Cylinders\",\"Displacement\",\"HP\",\"Weight\",\"Acceleration\",\"Year\",\"Origin\",\"Name\"])\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Get some info\n","df.info()"]},{"cell_type":"markdown","metadata":{},"source":["I'll set the two categorical values as categorical. This isn't required, but it will tell some things (e.g. pairplot, describe) to treat it as a categorical variable. \n","\n","Having correctly identified datatypes may or may not make a difference in terms of whether or not things will work, but it is good practice and will make some functions work more as we'd expect. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"89f88470ae298e1dffb13b17f50fb1f03c34d224","trusted":true},"outputs":[],"source":["# Converting the variables to the correct types\n","\n","#We'll need this once we notice the error below\n","#df = df[df[\"HP\"]!=\"?\"]\n","\n","df['Cylinders']=df['Cylinders'].astype('category')\n","df['Origin']=df['Origin'].astype('category')\n","df['HP']=df['HP'].astype('float64')\n","df.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cbb3e8d942041dcb5aa996969214811f1265f6ac","trusted":true},"outputs":[],"source":["#visualize pairplot\n","sns.pairplot(df)"]},{"cell_type":"markdown","metadata":{},"source":["There are only a few things that we might need to be attentive of in the pairplot. For the most part the data looks unremarkable:\n","<ul>\n","<li>Some colinearity between weight, displacement, and HP. We can address that later on. \n","<li>MPG (the target) looks to have a non-linear relationship with those varaibles. We'll consider things like that next time. \n","</ul> "]},{"cell_type":"markdown","metadata":{},"source":["We can look at boxplots and counts for the categorical varaibles. the countplot is basically just a simple hist for categorical data. \n","\n","We didn't know exactly what Origin was, but now I am going to suspect that it is American/Japanesse/European - we could likely verify this by looking at the data and using our domain knowledge of where different cars are from. \n","\n","The grid used below is the matplotlib way to do subplots, it is a sightly more elaborate version of the thinkplot one. We basically make a grid, then assign each graph to a square in the grid. The details of exactly how we visualize data don't really matter, as long as we graph it, but playing with some different ways gives us more tools to make things look OK. Googling \"seaborn ___________\" will almost always give some examples online - the seaborn stuff is much easier to use than the matplotlib directly, so I'd advise sticking to that. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"80bc8487d5edf304995ea0d438414ad85affed8d","trusted":true},"outputs":[],"source":["#Print boxplots - we'll put them in subplots to make it look fancy\n","#Countplots are basically categorical hists - we could've used a hist as well\n","fig, ax = plt.subplots(nrows=2,ncols=2,figsize=(20,12))\n","fig.suptitle(\"Categorical Plotting\", fontsize=35)\n","sns.boxplot(x=\"Cylinders\", y=\"MPG\", data=df,ax=ax[0,0])\n","sns.boxplot(x=\"Origin\", y=\"MPG\", data=df,ax=ax[0,1])\n","sns.countplot(x=\"Cylinders\", data=df,ax=ax[1,0])\n","sns.countplot(x=\"Origin\", data=df,ax=ax[1,1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[df[\"Cylinders\"]==3].Name.count(), df[df[\"Cylinders\"]==5].Name.count()"]},{"cell_type":"markdown","metadata":{},"source":["Looks like 3 and 5 cylinders are rare (if you konw about cars, this makes sense). I'm going to consider dropping those - we'll see. The reasoning is that we want to predict the MPG in general, there simply aren't very many samples for those two subgroups, so it may end up being more confounding than helpful. E.g. the impact of having 5 cylinders will be due to the 3 specific cars we have in the data, not the general impact of having that many cylinders. \n","\n","A similar example, say you were prediciting height of people and one factor was hair color. If you had 3 redheads in the data, any influence \"being a redhead\" has on the expected height would be overwhelmed by the influence that those specific 3 have. So that feature isn't giving you the \"impact of having red hair\" it is giving you the impact of \"being Jim\" - or the specific person in the sample. \n","\n","As well, if the data is being split, the results will probably be skewed - e.g. you may only get one 3 cylinder car in training the model - then the impact of 3 cylinders doesn't exist, you only get that one car. If you happen to get the one 3cyl car with terrible MPG, that will impact the results. If you got the most efficient 3cyl car ever in the next trial, things could then be totally different - the model would be unstable - bad. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"424f497e6b08b539644fcf76adc6c1593c3d4611","trusted":true},"outputs":[],"source":["## Correlation Matrix\n","corr = df.corr()\n","corr.style.background_gradient()\n","corr.style.background_gradient().set_precision(2)"]},{"cell_type":"markdown","metadata":{"_uuid":"7c61e2657205720959c84d5a54faed02958ca802"},"source":["The correlation matrix is very useful in Linear Regression. We know that if there is correlation between the varibles then we can't accurately attribute the impact between the correlated variables. \n","\n","Here, weight and displacement (the size of the engine) are highly correlated. We may remove one later on... \n","<br><br><br>\n","\n","<h2>Prep data for regression</h2>\n","\n","At this point we are pretty much ready to get our regression on. We just need to get the data in the right format. This will vary a little depending on what we want to use. We can also make some data cleaning choices relating to the issues we noticed above. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Clean out questionable bits - cylinders and correlated values. \n","#We can try once with this and once without. \n","#In this data, the difference probably won't be huge. \n","print(len(df))\n","df_ = df.drop(columns={\"Displacement\",\"Name\"})\n","df_ = df_[df_[\"Cylinders\"]!=3]\n","df_ = df_[df_[\"Cylinders\"]!=5]\n","\n","#We need to remove the categories that are not used anymore.\n","#They don't automatically vanish. \n","#The function from pandas need a series, so we have to do a roundabout way\n","df_['Cylinders'] = pd.Series(df_['Cylinders']).cat.remove_unused_categories()\n","print(len(df_))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Check the cylinders \n","df_[\"Cylinders\"].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["We have two categorical variables - cylinders and origin. Currently it will work with the numbers, but I don't think it is best. Right now they'll be treated as numbers in the regression, and that doesn't make a tonne of sense. \n","\n","To deal with them better, we can use one hot encoding to translate each category to its own column. We will do it the easy way with the get_dummies function. I'm going to do it twice, we'll explain why after. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Get some new info\n","df_.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"625b6881ab63a8e4384ead35a2d12442cc781c67","trusted":true},"outputs":[],"source":["# Do the dummies\n","df_tmp = pd.get_dummies(df_)\n","df_tmp.head(5)\n"]},{"cell_type":"markdown","metadata":{},"source":["Pretty simple, instead of each categorical column having several possible values, now each of those values is a column, and each row is true (or 1) in one of the columns and a false (or 0) for the others. \n","<br><br>\n","The one issue here is that what is represented if all the columns are 0? By giving each value its own column, we've invented possible data - all 0s. The solution to this is to just drop a column for each category - that one that is dropped is represented by all 0s now. By doing this we avoid inventing some data. This is common, though not universal. For linear regression we want to get rid of that. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2f9c9b1abd0601c62e13188a8cd5f5baebd877dc","trusted":true},"outputs":[],"source":["#Redo the dummy variables. \n","df2 = pd.get_dummies(df_, drop_first=True)\n","df2.head(5)\n"]},{"cell_type":"markdown","metadata":{"_uuid":"ab2c6eafab1f24fc829fe86e19b5df811cd6fc16"},"source":["The pandas function of creating the dummy variables is the most simplest one. The encoding notebook has an example of the sklearn process, but this way is much more simple. \n","\n","Next we want to split our features (Xs) and our target (Y). "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#note, whatever you make here will be the x/y in the split function below.\n","#We need a x and y array for sklearn.\n","#Having an x and y dataframe is also useful later on. \n","\n","#COMPLETE\n","\n","x.shape, y.shape"]},{"cell_type":"markdown","metadata":{},"source":["Data is ready to go - we can now either sklearn with the arrays or statsmodel with the dataframes or the arrays. \n","\n","We can split the data again into a training set and a test set, so we can calculate our accuracy on some new data. \n","\n","**Note: repeating the split/train/test/calculate cycle is often a good idea. We don't need to build a big loop (though we could) - we'll add this later on with a function built into sklearn that makes it easier - kfold.** "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8966443c9bed307758a8f430d8c3b866ee0176d8","trusted":true},"outputs":[],"source":["# Dividing the Dataset into Test and Train\n","# This does the splitting of both the xs and ys, and spits back all 4 sets. \n","# This function is really common, and does the same thing as the sample splitting we did by hand\n","from sklearn.model_selection import train_test_split\n","xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=.3)\n","\n","print(\"X-Train:\", xTrain.shape)\n","print(\"X-Test:\", xTest.shape)\n","print(\"Y-Train:\", yTrain.shape)\n","print(\"Y-Test:\", yTest.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b920e6b32552f4e2cfad3ed9d503f06f532fdefc","trusted":true},"outputs":[],"source":["# Implementing Linear Regression\n","# Fit the model\n","from sklearn.linear_model import LinearRegression\n","\n","#COMPLETE"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1367aebcba7a1ed066c265066d35fc8f37701f74","trusted":true},"outputs":[],"source":["## Seeing the result of Linear Regression\n","#I'll use the dataframes to make sure that we get the column names. \n","\n","# This one is using the post-split test data. Hence the lack of labels. \n","import statsmodels.api as sm\n","X2 = sm.add_constant(xTrain)\n","est = sm.OLS(yTrain,X2)\n","est2 = est.fit()\n","print(est2.summary())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"39ad605aecc742015fdf1e582c9752e74a84f5a0","trusted":true},"outputs":[],"source":["# Predictiions for TEST data\n","# Calculate RMSE for TEST data\n","# The sklearn and the statsmodels models are the same, so either works here. \n","\n","#COMPLETE\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Get Residuals and picture them in a DF for easy reading. \n","tmp1 = pd.DataFrame(yTest, columns={\"Y values\"})\n","tmp2 = pd.DataFrame(ypred, columns={\"Predictions\"})\n","tmp3 = pd.DataFrame((yTest-ypred), columns={\"Residual\"})\n","resFrame = pd.concat([tmp1,tmp2,tmp3], axis=1)\n","resFrame.sort_values(\"Residual\").head()"]},{"cell_type":"markdown","metadata":{},"source":["After we're done with all of this testing, what if we want to use the model to make predictions? We all just got jobs monitoring MPG of new cars, and we are way to lazy to measure all of them, so we want to make accurate predictions and go back to sleep. \n","\n","For the model we want to use, we'll train it with all the data. The splitting and testing gave us good estimates for accuracy, but using all the data to create the model will deliver the best results in general. What it won't do is give us those test scores to evaluate it, but if we've already decided to use it we should train a new model with all the data. In practice we'd probably compare the test scores that we got here with the scores we got using different algorithms, then we'd train the most accurate one with all data and use that. "]},{"cell_type":"markdown","metadata":{},"source":["<h2>Exercise - predict BMI</h2>\n","\n","Follow a similar process to predict BMI using the data below. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Load data\n","df_d = pd.read_csv(\"diabetes.csv\")\n","df_d.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Regress..."]}],"metadata":{"interpreter":{"hash":"ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"},"kernelspec":{"display_name":"Python 3.8.11 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":4}
