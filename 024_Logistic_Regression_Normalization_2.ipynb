{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression: StatsModels and Details of Regression</h1>\n",
    "\n",
    "We can do some more logitic regression to make classification predictions, there's some things that we do to try to drive accuracy up, and some other work we can do to interpret the accuracy better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read diabetes data\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Baseline Accuracy</h2>\n",
    "\n",
    "Before we get going, we'll touch on a simple concept - the baseline accuracy. For example, in post WW2 Germany the ratio of males to females was approximately .6 (3:5) for people in their 20s - or approximately 5/8 or 62.5% of people were females. \n",
    "\n",
    "If we were to build some model to predict if a twenty-soemthing was a male or female, this should be the worst we can do. A 'default' model of always guessing female will be 62.5% accurate, so if we can't beat that, we suck and are useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline for diabetes\n",
    "1 - df[\"Outcome\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... a model guessing NO at all times would be ~65% accurate here, so that's our worst case scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>StatsModels Formula Based Logistic Regression</h2>\n",
    "\n",
    "The book uses the forumla method for calling statsmodels, whereas when we did linear regression, we used the 'regular' way. For logistic regression, we can try using the formula here. As with linear regression, statsmodels gives us some more detailed data that we can look at to try to understand the effectiveness of the model, such as p values for each feature. \n",
    "\n",
    "Like linear regression, scikitlearn vs statsmodels vs any other library function is pretty much up to you. They do the same thing, so use what you please. SKlearn is a little more transferable to future uses in terms of mechanics. \n",
    "\n",
    "With the statsmodels formula, we can save a little bit of data manipulation in exchange for writing out the feature names. Most notably, we can keep the Xs and Y together in the data, then when we write the formula, that does the 'splitting'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data - we are keeping the DF to make the results nice\n",
    "#If doing a dataframe, there's no x/y split. So we basically cut the function in half\n",
    "test1, train1 = train_test_split(df,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statsmodels Formula</h3>\n",
    "\n",
    "The formual for statsmodels is relatively simple. The format is:\n",
    "\n",
    "Y ~ x1 + x2 + x3....\n",
    "\n",
    "Then we feed the logit formula the dataframe to use as well as the formula that tells it which columns go where. After that, it is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula\n",
    "form = \"Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age\"\n",
    "model1 = sm.logit(data=train1, formula=form).fit()\n",
    "\n",
    "#Make predictions for later, get summary for now\n",
    "preds1 = model1.predict(test1)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the head of the predictions. \n",
    "preds1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions give us the probabilities, we need to convert to 0-1 to give 'real' answers. We'll make a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the probabilities to yes/no correctness\n",
    "def convert_prob_to_label(prob, cutoff = 0.5):\n",
    "    label = []\n",
    "    for i in range(len(prob)):\n",
    "        if prob[i] > cutoff:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = convert_prob_to_label(np.array(preds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(test1[\"Outcome\"], labels)\n",
    "sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create calc for misclassification rate. \n",
    "#We can look this up, but it is just adding up the errors and dividing. \n",
    "#The confusion matrix is an array, so we just need to grab the correct cells. \n",
    "#The is the compliment to the accuracy score, so we can just use library functions in general\n",
    "mis_rate = (conf_matrix[[1],[0]].flat[0] + conf_matrix[[0],[1]].flat[0])/len(test1)\n",
    "print(mis_rate)\n",
    "print(accuracy_score(test1[\"Outcome\"], labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the accuracy is ok, but we have a noticable imbalance between false negatives and false positives (exact split we bounce around due to randomness - when I built this the frist run was pretty large, other runs were smaller). We previously looked briefly at all the different accuracy metrics that can be generated from the confusion matrix. In real use, you need to combine some domain knowledge with these results to choose the measures that make sense for what you're doing. \n",
    "\n",
    "<h3>F1 Score</h3>\n",
    "\n",
    "There is one score that is pretty common that may be helpful  - the F1 score. The F1 score seeks to balance PRECISION (avoid false positives) and RECALL (target true positives, at cost of false positives). There's also a way (fbeta) to adjust the balance between recall and precision, but we'll set that aside for now. The f1 score is:\n",
    "\n",
    "F1 = 2 * ( (precision * recall) / (precision + recall) )\n",
    "\n",
    "In practice, it is simple with an sklearn function. \n",
    "\n",
    "<h3>Side Note: Other Metrics</h3>\n",
    "\n",
    "Below the F1, I put in log-loss, which is another metric of accuracy. This one is commonly seen later on when doing neural network stuff. The idea is always the same - we want to find something that minimizes the amount of error. In a question on the credit card fraud I said that you may try to optimize for some other metric than raw accuracy - this is an example of one that is common. In short, you'll define a ML algorithm to use, then define a loss function (e.g. log-loss), and the algorithm will repeat trials (gradient descent) in an effort to minimize that \"loss\". \n",
    "\n",
    "Log Loss is the negative average of the log of corrected predicted probabilities for each instance. E.g. if a value is true, and a prediction is .8 probability, the corrected probability is .8; if a value is false and the prediction is .8 probability, the corrected proability is .2. These values are \"logged\", then the negative average is taken (the logs are negative), and that's the metric for loss - or how bad the predictions are. Less loss, more accuracy. This is also known as cross-entropy loss. Think back to the original look we took at error in the first look at logistic regression. \n",
    "\n",
    "This is what the internal process of the logistic regression is actually minimizing on each trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also use the accuracy score for sklearn. \n",
    "print(\"Acc%:\", accuracy_score(test1[\"Outcome\"], labels))\n",
    "print(\"F1:\", f1_score(test1[\"Outcome\"], labels))\n",
    "print(\"LogLoss:\", log_loss(test1[\"Outcome\"], preds1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, roughly 77% accuracy. Put that in our pockets. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Scaling</h2>\n",
    "\n",
    "Feature scaling includes several similar methods of taking numerical values and transforming them to be on a different scale. There are several, a few we'll look at right now are:\n",
    "<ul>\n",
    "<li>Normalization - rescale the data so all values are between 0 and 1. \n",
    "<li>Standardization - rescale the data so there is a mean of 0 and a standard deviation of 1. \n",
    "</ul>\n",
    "\n",
    "These all process our data in a similar way - taking the original data, and shifting its distribution using a transformation. The calculations for these scaling methods are:\n",
    "<ul>\n",
    "<li>Normalization: x = ( (x-min(x)) / (max(x)-min(x)) )\n",
    "<li>Standardization: x = ( (x-mean(x)) / (std(x)) )\n",
    "</ul>\n",
    "\n",
    "Note: There are other feature scaling algorithms/methods, these are just the two more common ones. The idea is pretty much always the same. One other consdieration is how the scaling treats outliers, which we'll worry about more next semester. \n",
    "\n",
    "<h3>OK.... Why?</h3>\n",
    "\n",
    "Feature scaling has several benefits, the impact of these benefits varies widely depending on the exact original data, and the type of models you're using. The reasons are:\n",
    "<ul>\n",
    "<li>Scaling - different values may have widely different scales (e.g. if processing a loan, age and net worth will be very different). Scaling can sometimes cause problems with the relative impact of different ranges distorting calculations. Small values can be \"drowned out\" by larger values. \n",
    "<li>Range - similar to the scale problem, if data values are radically different, some calculations will become less accurate. This particularly can impact distance based calculations, like clustering. \n",
    "<li>Speed - with algorithms that use methods like gradient descent (like logistic regression), having values on different scales and ranges may cause the algorithm to take longer to converge on a solution, or potentially prevent it at all. \n",
    "</ul>\n",
    "\n",
    "Feature scaling can improve predictive accuracy, sometimes dramatically. Scaling data is not super impactful in some calculations (linear regression, trees) and it can (can - not will) be very impactful in others (logistic regression, gradient descent, neural networks, PCA). In general, when we have an algorithm that is adjusting itself to try to find the most accurate solution (e.g. log. reg. with gradient descent), scaling the data helps and is pretty standard. When we get to things like neural networks later on, it is just part of the process. \n",
    "\n",
    "We will do this pretty often in machine learning applications, similar to encoding categorical varaibles it is kind of a preparatory step that we just do without thinking much about it because it makes things work (or work better).\n",
    "\n",
    "<h3>Which Scaler to Pick?</h3>\n",
    "\n",
    "There is not generally a definitive answer to that, and the real answer is to try a few and observe the results in accuracy. We do have a few rules of thumb:\n",
    "<ul>\n",
    "<li>Normalization: distribution is unkonwn, things need to be 0 to 1. \n",
    "<li>Standardization: distributions are normal. \n",
    "</ul>\n",
    "\n",
    "For now deciding between the methods isn't a huge concern. If the features look normal, we'll try to standardize; if not, normalize. We can worry about some finer differences as they come up next semester. If in doubt, try each, check accuracy, choose the best. Outliers will impact each (think about why), so we probably want to deal with those prior to scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, we can build a function for each, since they are simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some example data\n",
    "#d1 = [1,2,3,4,5,9,5,12,7,8,12,5,6,8,2,8,9]\n",
    "#d2 = [1,2,3,4,5,9,5,12,7,12,5,6,8,2,8,9,800]\n",
    "d1 = df[\"BMI\"].to_list()\n",
    "d2 = df[\"Glucose\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETE FUNCTION\n",
    "#def dumbNormalizer(x):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize some data\n",
    "tmp = dumbNormalizer(d1)\n",
    "print(np.mean(tmp))\n",
    "print(tmp[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a dumb standardizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETE FUNCTION\n",
    "#def dumbStandardizer(x):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize some data\n",
    "tmp = dumbStandardizer(d1)\n",
    "print(np.mean(tmp))\n",
    "print(tmp[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How Does the Distribution Change?</h3>\n",
    "\n",
    "We can visualize both datasets in their original form, after normalization, and after standardization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize all\n",
    "thinkplot.PrePlot(6,2,3)\n",
    "sns.histplot(d1, stat=\"density\")\n",
    "thinkplot.SubPlot(2)\n",
    "sns.histplot(dumbNormalizer(d1), stat=\"density\")\n",
    "thinkplot.SubPlot(3)\n",
    "sns.histplot(dumbStandardizer(d1), stat=\"density\")\n",
    "thinkplot.SubPlot(4)\n",
    "sns.histplot(d2, stat=\"density\")\n",
    "thinkplot.SubPlot(5)\n",
    "sns.histplot(dumbNormalizer(d2), stat=\"density\")\n",
    "thinkplot.SubPlot(6)\n",
    "sns.histplot(dumbStandardizer(d2), stat=\"density\")\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scaling Outcomes</h3>\n",
    "\n",
    "For each of the scaling, the distribution is the same for the data after the transformation, but the range on the x scale is different. We can also see visually the impact of outliers here - we'd normally want to address those in advance. In extreme cases, having a massive outlier can \"squish\" all the data at one end of the distribution, which will be bad in most cases. \n",
    "\n",
    "<h4>Scaling Caveat</h4>\n",
    "\n",
    "One detail we've overlooked here is that the data should technically be scaler after the split of train/test data, and the scaler should only be trained on the training data (fit) and applied on the testing data (transform). This is to prevent any data leakage - the test data is supposed to be brand new, and if it is able to influence the scaling, then it has some impact on the training of the model. How much of an impact does this make? Probabaly not much for the vast majority of applications, but it is technically correct to keep 'em separated. \n",
    "\n",
    "We'll do the below example with it scaled 'properly'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Library Scaling Functions</h3>\n",
    "\n",
    "Luckily, we don't really need to bust out our algebra to build any of these calculations by hand, we can use some built in functions in scikit learn to do it for us. These functions can also be built into a pipeline to process data (next semester), so we can build this transformation in without really seeing the resutlts. Like encoding last week, this takes data that is readable, and makes it not readable - we can package all of those steps in with our modelling, so legible data goes in - a trained model comes out. \n",
    "\n",
    "In scikit learn, the different functions for these methods are (examples below in code):\n",
    "<ul>\n",
    "<li>Normalization: MinMaxScaler\n",
    "<li>Standardization: StandardScaler\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example with Real Functions</h3>\n",
    "\n",
    "Building scaling functions is pretty easy, in practice though we can use the sklearn ones, lets build that into the logistic regression for the diabetes prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kill outliers. \n",
    "#We'll use this in a bit. \n",
    "\n",
    "#df = df[df[\"Glucose\"] > 20]\n",
    "#df = df[df[\"BloodPressure\"] > 20]\n",
    "#df = df[df[\"SkinThickness\"] < 80]\n",
    "#df = df[df[\"Glucose\"] > 20]\n",
    "#df = df[df[\"BMI\"] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make arrays from data\n",
    "# Y = Outcome\n",
    "# Xs = the other stuff\n",
    "dfY = df[\"Outcome\"]\n",
    "dfX = df.drop(columns={\"Outcome\"}) #There isn't generally a need to standardize the Y values\n",
    "\n",
    "x = np.array(dfX)\n",
    "y = np.array(dfY).reshape(-1,1)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create scaler and choose method. \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Comment one out, use the other\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize or normalize\n",
    "#Use training data to fit the scaler, then apply that predefined scale to the test data\n",
    "x2 = scaler.fit_transform(X_train2)\n",
    "X_trans2 = scaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have standardized and normalized data, do linear regression stuff...\n",
    "\n",
    "I'm going to set it up so we can swap the x data in the first line of code below, mostly to save typing. We can run everything twice, once for each scaling technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "model2 = LogisticRegression().fit(x2,y_train2.ravel())\n",
    "\n",
    "#Make predictions\n",
    "preds2 = model2.predict(X_trans2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 1/0 results and show results\n",
    "labels2 = convert_prob_to_label(np.array(preds2))\n",
    "conf_matrix2 = confusion_matrix(y_test2,labels2)\n",
    "sns.heatmap(conf_matrix2, annot=True)\n",
    "\n",
    "print(\"F1:\",f1_score(y_test2, labels2))\n",
    "print(\"Acc:\",accuracy_score(y_test2, labels2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay. We are super happy. \n",
    "\n",
    "IIRC, the accuracy without doing any scaling was similar (~76%), that's ok. This data didn't really have any massively differing ranges for the original data. We also didn't do any outlier filtering - what if we try that and repeat? We know from doing the dumb scaling, that there are a few outliers. \n",
    "\n",
    "There's a simple filter commented out up above, we'll go remove and repeat. It might make it better, it might not. This case only has a few outliers and they are not outrageous, so we shouldn't be surprised if the difference is relatively small. We can see that the errors are more balanced, so the F1 score does show improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Logistic Regression with Normalization Exercise</H1>\n",
    "\n",
    "Logistic regression with normalization. \n",
    "\n",
    "Identify penguin sex. In doing so:\n",
    "<ul>\n",
    "<li>Explore the data. \n",
    "<li>Clean any erroneous data. \n",
    "<li>Create classification model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sns.load_dataset(\"penguins\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF Info/describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical value exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical things look pretty OK. What about categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Facetgrid allows us to explore and split data easily. \n",
    "#Could play with this for differnet views. https://seaborn.pydata.org/generated/seaborn.FacetGrid.html \n",
    "#Col and row in the first line are our categoricals.\n",
    "#Second line is the type of chart, and its relevent details. \n",
    "g = sns.FacetGrid(df2, col=\"species\", row=\"island\")\n",
    "g.map_dataframe(sns.scatterplot, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing looks too odd in the exploration (this is a toy dataset, so that's normal). We can do some modelling and predict sex...\n",
    "\n",
    "I will sklearn, because I like it more. Statsmodels imight be useful in a bit, we could throw it in to get p values for the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the categorical varaiables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make arrays from new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "#scalerP = StandardScaler()\n",
    "scalerP = MinMaxScaler()\n",
    "\n",
    "#Use training data to fit the scaler, then apply that predefined scale to the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare to baseline accuracy\n",
    "df2[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline is roughly 50/50, how'd we do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions and examine accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing pretty well. What could be better? \n",
    "\n",
    "One suspicion I have would be that the species are somewhat different. Gentoo in particular seems to have different metrics, so maybe we'd predict it separately? I might need more data for that. \n",
    "\n",
    "Also, can we improve by removing confounding vars? Look at StatsModels to have an idea of variable relevance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode all data\n",
    "dfP_ = pd.get_dummies(data=df2, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm not going to split data, all I want is variable importance, I'm not evaluating accuracy here. \n",
    "formP = \"sex_Male ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + species_Chinstrap + species_Gentoo + island_Dream + island_Torgersen\"\n",
    "modelP_SM = sm.logit(data=dfP_, formula=formP).fit()\n",
    "modelP_SM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll remove island and flipper length, try again, and see what's up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the things that look least useful. \n",
    "dfXP2 = df2_.drop(columns={\"sex_Male\", \"flipper_length_mm\", \"island_Dream\", \"island_Torgersen\"}) \n",
    "\n",
    "#Make arrays from new data, check shapes\n",
    "xP2 = np.array(dfXP2)\n",
    "xP2.shape, yP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "\n",
    "\n",
    "#Use training data to fit the scaler, then apply that predefined scale to the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions and examine accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!\n",
    "\n",
    "Note: We have relatively little data, so the swings for accuracy between trials can be pretty pronounced. The first time I ran this there was a big improvement in accuracy after removing stuff, other runs had a smaller difference. More data would make it more stable, repeating the trials would also. You could build a loop to repeat trials, we'll do it the sklearn way early next semester. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
